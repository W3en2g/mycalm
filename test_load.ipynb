{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pcwen/miniconda3/envs/calm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.22s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.79s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CALM:\n\tsize mismatch for anchor_model.model.embed_tokens.weight: copying a param with shape torch.Size([333839968]) from checkpoint, the shape in current model is torch.Size([152064, 3584]).\n\tsize mismatch for anchor_model.model.norm.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([3584]).\n\tsize mismatch for anchor_model.lm_head.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([152064, 3584]).\n\tsize mismatch for aug_model.model.embed_tokens.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([151936, 896]).\n\tsize mismatch for aug_model.model.norm.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([896]).\n\tsize mismatch for aug_model.lm_head.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([151936, 896]).\n\tsize mismatch for cross_attention_hooks.0.proj.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([3584, 896]).\n\tsize mismatch for cross_attention_hooks.0.proj.bias: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([3584]).\n\tsize mismatch for cross_attention_hooks.0.post_attention_layernorm.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([3584]).\n\tsize mismatch for cross_attention_hooks.0.cross_attention.in_proj_weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([10752, 3584]).\n\tsize mismatch for cross_attention_hooks.0.cross_attention.in_proj_bias: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([10752]).\n\tsize mismatch for cross_attention_hooks.0.cross_attention.out_proj.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([3584, 3584]).\n\tsize mismatch for cross_attention_hooks.0.cross_attention.out_proj.bias: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([3584]).\n\tsize mismatch for cross_attention_hooks.1.proj.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([3584, 896]).\n\tsize mismatch for cross_attention_hooks.1.proj.bias: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([3584]).\n\tsize mismatch for cross_attention_hooks.1.post_attention_layernorm.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([3584]).\n\tsize mismatch for cross_attention_hooks.1.cross_attention.in_proj_weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([10752, 3584]).\n\tsize mismatch for cross_attention_hooks.1.cross_attention.in_proj_bias: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([10752]).\n\tsize mismatch for cross_attention_hooks.1.cross_attention.out_proj.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([3584, 3584]).\n\tsize mismatch for cross_attention_hooks.1.cross_attention.out_proj.bias: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([3584]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m modelpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalm_saves/qwen_mixed_1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m config \u001b[38;5;241m=\u001b[39m calm\u001b[38;5;241m.\u001b[39mCALMConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(modelpath)\n\u001b[0;32m----> 4\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mcalm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCALM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# loaded_model = calm.CALM.from_pretrained(modelpath, config = config)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n",
      "File \u001b[0;32m~/miniconda3/envs/calm/lib/python3.10/site-packages/transformers/modeling_utils.py:3838\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3829\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3831\u001b[0m     (\n\u001b[1;32m   3832\u001b[0m         model,\n\u001b[1;32m   3833\u001b[0m         missing_keys,\n\u001b[1;32m   3834\u001b[0m         unexpected_keys,\n\u001b[1;32m   3835\u001b[0m         mismatched_keys,\n\u001b[1;32m   3836\u001b[0m         offload_index,\n\u001b[1;32m   3837\u001b[0m         error_msgs,\n\u001b[0;32m-> 3838\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3845\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3846\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3849\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3850\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3855\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3857\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3858\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/miniconda3/envs/calm/lib/python3.10/site-packages/transformers/modeling_utils.py:4349\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize mismatch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_msg:\n\u001b[1;32m   4346\u001b[0m         error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4347\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4348\u001b[0m         )\n\u001b[0;32m-> 4349\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unexpected_keys) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   4352\u001b[0m     archs \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39marchitectures \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39marchitectures\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CALM:\n\tsize mismatch for anchor_model.model.embed_tokens.weight: copying a param with shape torch.Size([333839968]) from checkpoint, the shape in current model is torch.Size([152064, 3584]).\n\tsize mismatch for anchor_model.model.norm.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([3584]).\n\tsize mismatch for anchor_model.lm_head.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([152064, 3584]).\n\tsize mismatch for aug_model.model.embed_tokens.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([151936, 896]).\n\tsize mismatch for aug_model.model.norm.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([896]).\n\tsize mismatch for aug_model.lm_head.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([151936, 896]).\n\tsize mismatch for cross_attention_hooks.0.proj.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([3584, 896]).\n\tsize mismatch for cross_attention_hooks.0.proj.bias: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([3584]).\n\tsize mismatch for cross_attention_hooks.0.post_attention_layernorm.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([3584]).\n\tsize mismatch for cross_attention_hooks.0.cross_attention.in_proj_weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([10752, 3584]).\n\tsize mismatch for cross_attention_hooks.0.cross_attention.in_proj_bias: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([10752]).\n\tsize mismatch for cross_attention_hooks.0.cross_attention.out_proj.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([3584, 3584]).\n\tsize mismatch for cross_attention_hooks.0.cross_attention.out_proj.bias: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([3584]).\n\tsize mismatch for cross_attention_hooks.1.proj.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([3584, 896]).\n\tsize mismatch for cross_attention_hooks.1.proj.bias: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([3584]).\n\tsize mismatch for cross_attention_hooks.1.post_attention_layernorm.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([3584]).\n\tsize mismatch for cross_attention_hooks.1.cross_attention.in_proj_weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([10752, 3584]).\n\tsize mismatch for cross_attention_hooks.1.cross_attention.in_proj_bias: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([10752]).\n\tsize mismatch for cross_attention_hooks.1.cross_attention.out_proj.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([3584, 3584]).\n\tsize mismatch for cross_attention_hooks.1.cross_attention.out_proj.bias: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([3584]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method."
     ]
    }
   ],
   "source": [
    "from model import calm\n",
    "modelpath = \"calm_saves/qwen_mixed_1\"\n",
    "config = calm.CALMConfig.from_pretrained(modelpath)\n",
    "loaded_model = calm.CALM.from_pretrained(modelpath, config = config)\n",
    "# loaded_model = calm.CALM.from_pretrained(modelpath, config = config)\n",
    "\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "\n",
    "prompt = \"My name is\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "generate_ids = loaded_model.generate(inputs.input_ids, max_length=100)\n",
    "print(tokenizer.decode(generate_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pcwen/miniconda3/envs/calm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.29s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.08s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from model import calm\n",
    "# modelpath = \"calm_saves/native_model_mixed\"\n",
    "modelpath = \"calm_saves/qwen_mixed/checkpoint-2000\"\n",
    "\n",
    "config = calm.CALMConfig.from_pretrained(modelpath)\n",
    "# loaded_model = calm.CALM.from_pretrained(modelpath, config = config,ignore_mismatched_sizes=True)\n",
    "loaded_model = calm.CALM.from_pretrained(modelpath, config = config)\n",
    "\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如果一个未满16岁的少年犯需要被监禁怎么办？,1etrofit16ujący犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯犯\n"
     ]
    }
   ],
   "source": [
    "prompt = \"如果一个未满16岁的少年犯需要被监禁怎么办？\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\",truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "generate_ids = loaded_model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_length=500,\n",
    "    temperature=0.7,  # 控制生成的随机性，值越高越随机\n",
    "    top_k=50,         # 限制选择的词汇在前k个概率最高的词中\n",
    "    top_p=0.9,        # 使用核采样，选择累计概率达到p的词汇\n",
    "    do_sample=True    # 启用采样\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(generate_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pcwen/miniconda3/envs/calm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.25s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.13s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "明白了。如果被控勒索罪，应该如何应对呢？ Ashe俗)$_idthôt然是不是只限于第27条法律原则吗？安全)$_\n",
      "\n",
      "风雨淋漓者是指什么啊？\n",
      "\n",
      "\n",
      "otta式法律文书是指什么方面呢？真是非常需要臾/moment性地？\n",
      "\n",
      "\n",
      "\n",
      "(ii) 埢然需要考虑些什么方面呢？\n",
      "\n",
      "\n",
      "*'需要indsay式'是指什么方面？\n",
      "\n",
      "\n",
      "需要考虑的是什么方面呢？一般情况下，需要考虑的是什么呢？\n",
      "\n",
      "\n",
      "\n",
      "elfast地停车时间是指什么方面？\n",
      "\n",
      "\n",
      "一般情况下，各机构需要考虑的是什么方面呢？\n",
      "\n",
      "\n",
      "⊆️ 榁然？\n",
      "\n",
      "\n",
      "ucz然 <<\n",
      "根本需要考虑的是什么方面呢？\n",
      "\n",
      "\n",
      "通常需要考虑的是什么方面的问题呢？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "*'严重伤害性'是指什么意思呢？\n",
      "\n",
      "\n",
      "⊆erah性侵害是指什么方面的问题呢？如果需要停车前先\n",
      "\n",
      "┴.offsetHeight是怎态？\n",
      "\n",
      "\n",
      "⊆\n",
      "\n",
      ":checked的是什么方面的问题呢？有关于超龄者、孕妇、中年人和儿童等？\n",
      "\n",
      "\n",
      "www.elfast地主要是指什么意思？\n",
      "\n",
      "\n",
      "更具体地说，'严重伤害者'是指什么意思，通常是指什么方面呀？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "⊆GameManager中\n",
      "\n",
      "⊆ Cedar式、全面性自卫\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model import calm\n",
    "\n",
    "modelpath = \"calm_saves/qwen_mixed_aug_fired\"\n",
    "config = calm.CALMConfig.from_pretrained(modelpath)\n",
    "loaded_model = calm.CALM.from_pretrained(modelpath, config=config)\n",
    "\n",
    "# 将模型移到GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loaded_model.to(device)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
    "\n",
    "prompt = \"明白了。如果被控勒索罪，应该如何应对呢？\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "# 将输入移到GPU\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "generate_ids = loaded_model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=500,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(generate_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:29<00:00,  7.45s/it]\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "怎么领养孩子？ 领养孩子通常需要经过一系列的程序和步骤。这些程序可能因国家和地区而异，但通常包括以下步骤：\n",
      "\n",
      "1. 咨询和评估：首先，您需要咨询当地的领养机构或社会服务机构，了解有关领养的法律和程序。您可能需要参加咨询会议，与社工进行面谈，并填写申请表格。\n",
      "\n",
      "2. 评估：社工将对您进行评估，以确保您适合\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "prompt = \"怎么领养孩子？\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=100)\n",
    "print(tokenizer.decode(generate_ids[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
